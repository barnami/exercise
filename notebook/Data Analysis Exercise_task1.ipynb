{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Analyze the store visitation by date and affinity profile of store visitors.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "import shapely.wkt\n",
    "from shapely.geometry import Point, Polygon\n",
    " \n",
    "def gps_signals_import(sample_or_full):\n",
    "    # Lists all files in the given directory, reads the csv files, and append to a new Dataframe\n",
    "    if (sample_or_full == 'sample'):\n",
    "        workdir = \"c:\\\\Users\\\\barna\\\\Downloads\\\\adsquare\\\\assignment_data\\\\sample_data\\\\\"\n",
    "    elif (sample_or_full == 'full'):\n",
    "        workdir = \"c:\\\\Users\\\\barna\\\\Downloads\\\\adsquare\\\\assignment_data\\\\full_data\\\\\"  \n",
    "    all_files = os.listdir(workdir)\n",
    "    gps = pd.DataFrame()\n",
    "    for a in range(len(all_files)):\n",
    "        file = all_files[a]\n",
    "        data = pd.read_csv(os.path.join(workdir, file))\n",
    "        gps = gps.append(data)\n",
    "        print(\"File name:\\tFile length:\\tDatabase length:\\n\"+file,'\\t',len(data),'\\t'+'\\t',len(gps))\n",
    "    return gps\n",
    "\n",
    "def store_data_import():\n",
    "    workdir = \"c:\\\\Users\\\\barna\\\\Downloads\\\\adsquare\\\\assignment_data\\\\\"\n",
    "    file = 'stores.csv'\n",
    "    data = pd.read_csv(os.path.join(workdir, file))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name:\tFile length:\tDatabase length:\n",
      "part_1.csv \t 40000 \t\t 40000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_10.csv \t 40000 \t\t 80000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_11.csv \t 40000 \t\t 120000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_12.csv \t 40000 \t\t 160000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_13.csv \t 40000 \t\t 200000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_14.csv \t 40000 \t\t 240000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_15.csv \t 40000 \t\t 280000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_16.csv \t 40000 \t\t 320000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_17.csv \t 40000 \t\t 360000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_18.csv \t 40000 \t\t 400000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_19.csv \t 40000 \t\t 440000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_2.csv \t 40000 \t\t 480000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_20.csv \t 40000 \t\t 520000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_21.csv \t 40000 \t\t 560000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_22.csv \t 40000 \t\t 600000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_23.csv \t 40000 \t\t 640000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_24.csv \t 40000 \t\t 680000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_25.csv \t 40000 \t\t 720000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_26.csv \t 40000 \t\t 760000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_27.csv \t 40000 \t\t 800000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_28.csv \t 40000 \t\t 840000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_29.csv \t 40000 \t\t 880000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_3.csv \t 40000 \t\t 920000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_30.csv \t 40000 \t\t 960000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_31.csv \t 40000 \t\t 1000000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_32.csv \t 40000 \t\t 1040000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_33.csv \t 40000 \t\t 1080000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_34.csv \t 40000 \t\t 1120000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_35.csv \t 40000 \t\t 1160000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_36.csv \t 40000 \t\t 1200000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_37.csv \t 40000 \t\t 1240000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_38.csv \t 40000 \t\t 1280000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_39.csv \t 40000 \t\t 1320000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_4.csv \t 40000 \t\t 1360000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_40.csv \t 40000 \t\t 1400000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_41.csv \t 40000 \t\t 1440000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_42.csv \t 40000 \t\t 1480000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_43.csv \t 40000 \t\t 1520000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_44.csv \t 40000 \t\t 1560000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_45.csv \t 40000 \t\t 1600000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_46.csv \t 40000 \t\t 1640000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_47.csv \t 40000 \t\t 1680000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_48.csv \t 40000 \t\t 1720000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_49.csv \t 40000 \t\t 1760000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_5.csv \t 40000 \t\t 1800000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_50.csv \t 40000 \t\t 1840000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_51.csv \t 40000 \t\t 1880000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_52.csv \t 40000 \t\t 1920000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_53.csv \t 40000 \t\t 1960000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_54.csv \t 40000 \t\t 2000000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_55.csv \t 40000 \t\t 2040000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_56.csv \t 40000 \t\t 2080000\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_57.csv \t 22913 \t\t 2102913\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_6.csv \t 40000 \t\t 2142913\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_7.csv \t 40000 \t\t 2182913\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_8.csv \t 40000 \t\t 2222913\n",
      "File name:\tFile length:\tDatabase length:\n",
      "part_9.csv \t 40000 \t\t 2262913\n"
     ]
    }
   ],
   "source": [
    "# import GPS signals from sample or full datasets\n",
    "signals = gps_signals_import(sample_or_full='sample')\n",
    "# sort by utc_timestamp\n",
    "signals = signals.sort_values(by=[\"utc_timestamp\"]).reset_index(drop=True)\n",
    "# get the date from utc_timestamp\n",
    "signals[\"date\"] = signals[\"utc_timestamp\"].astype(\"datetime64[ms]\").dt.to_period(\"D\")\n",
    "# transform the coordinates to a GeoDataFrame geometry\n",
    "signals_gdf = gpd.GeoDataFrame(signals, geometry=gpd.points_from_xy(signals[\"lon\"], signals[\"lat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stores data\n",
    "stores = store_data_import()\n",
    "# transform strings to geometry format\n",
    "stores[\"wkt\"] = stores[\"wkt\"].apply(lambda x: shapely.wkt.loads(x))\n",
    "# rename 'wkt' column to 'geometry'\n",
    "stores = stores.rename(columns={\"wkt\": \"geometry\"})\n",
    "# transform stores data into a GeoDataFrame\n",
    "stores_gdf = gpd.GeoDataFrame(stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create users from unique device_ids\n",
    "users = signals[[\"device_id\"]]\n",
    "users = users.drop_duplicates().sort_values(by=['device_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import affinities and merge with user (device) ids\n",
    "workdir = \"c:\\\\Users\\\\barna\\\\Downloads\\\\adsquare\\\\assignment_data\\\\affinities\\\\\"\n",
    "all_files = os.listdir(workdir)\n",
    "\n",
    "user_affinities = users.copy()\n",
    "for a in range(len(all_files)):\n",
    "    data = pd.read_csv(os.path.join(workdir, all_files[a]), header=None)\n",
    "    data = data.rename(columns={0 : \"device_id\"})\n",
    "    data.insert(1, all_files[a], 1)\n",
    "    user_affinities = pd.merge(left=user_affinities, right=data, how='left', left_on='device_id', right_on='device_id').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "a. Resolve the user visits per store, i.e. filter the GPS signals through polygons.\n",
    "\"\"\"\n",
    "# join signals and stores with geopandas Spatial Joins\n",
    "join = gpd.sjoin(signals_gdf, stores_gdf, how=\"inner\", op=\"within\")\n",
    "join = join.reset_index(drop=True)\n",
    "# merge user visits with user affinities\n",
    "join_with_user_affinities = pd.merge(left=join, right=user_affinities, how='left', left_on='device_id', right_on='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "b. Group the resolved visits by date (yyyy-mm-dd), store_name, and store_id.\n",
    "c. For each store_id/store_name/date provide the following metric.\n",
    "c - i. A total number of GPS signals per place_id/date.\n",
    "c - ii. A total number of unique visitors (i.e. device ids)\n",
    "c - iii. A total number of unique visitors belonging to each affinity group\n",
    "\"\"\"\n",
    "\n",
    "gps_total = join_with_user_affinities.groupby(by=[\"date\", \"store_name\", \"store_id\"]).agg({\"utc_timestamp\": \"count\"}).rename(columns={\"utc_timestamp\": \"total_signals\"}).reset_index()\n",
    "gps_unique = join_with_user_affinities.groupby(by=[\"date\", \"store_name\", \"store_id\"]).agg({\"device_id\": \"nunique\"}).rename(columns={\"device_id\": \"unique_visits\"}).reset_index()\n",
    "columns = ['date', 'store_name', 'store_id']\n",
    "gps_unique.drop(columns, inplace=True, axis=1)\n",
    "\n",
    "gps_unique_aff = join_with_user_affinities.drop_duplicates(subset=[\"device_id\", \"date\", \"store_id\", \"store_name\"]).groupby(by=[\"date\", \"store_name\", \"store_id\"]).agg(sum).reset_index()\n",
    "columns = ['device_id', 'lat', 'lat', 'lon', 'utc_timestamp', 'index_right', 'date', 'store_name', 'store_id']\n",
    "gps_unique_aff.drop(columns, inplace=True, axis=1)\n",
    "\n",
    "# concat the 3 dataframes\n",
    "group_by = pd.concat([gps_total, gps_unique, gps_unique_aff], axis = 1)\n",
    "\n",
    "# out files:\n",
    "workdir = \"c:\\\\Users\\\\barna\\\\Downloads\\\\adsquare\\\\assignment_data\\\\out\\\\\"\n",
    "# all store visit:\n",
    "join_with_user_affinities.to_csv(os.path.join(workdir, 'all_store_visit.csv'), index=False)\n",
    "# store visit group by date (yyyy-mm-dd), store_name, and store_id\n",
    "group_by.to_csv(os.path.join(workdir, 'store_visit_groupby.csv'), index=False)\n",
    "# stores data\n",
    "stores_gdf.to_file(os.path.join(workdir, 'stores_gdf.geojson'), driver=\"GeoJSON\")\n",
    "# all store visit gps\n",
    "all_gps = join_with_user_affinities[['device_id', 'geometry']]\n",
    "all_gps.to_file(os.path.join(workdir, 'all_gps.geojson'), driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
